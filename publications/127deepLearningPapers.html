<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export (no Abstract/BibTeX)
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();

	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);

	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array();

	// get data from each row
	entryRowsData = new Array();

	BibTeXKeys = new Array();

	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
}

function quickSearch(){

	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }

		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}

	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false;

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){
				found = true;
			}

			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)

	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents
var stripstring =
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com

	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');

	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="5%">Reftype</th><th width="5%">Citations</th></tr></thead>
<tbody>
<tr id="amodei2015deep" class="entry">
	<td>Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates, A., Diamos, G. and others</td>
	<td>Deep speech 2: End-to-end speech recognition in english and mandarin</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1512.02595&nbsp;</td>
	<td>article</td>
	<td>175</td>
</tr>

<tr id="andrychowicz2016learning" class="entry">
	<td>Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.W., Pfau, D., Schaul, T. and de Freitas, N.</td>
	<td>Learning to learn by gradient descent by gradient descent</td>
	
	<td>2016</td>
	<td>Advances in Neural Information Processing Systems, pp. 3981-3989&nbsp;</td>
	<td>inproceedings</td>
	<td>40</td>
</tr>

<tr id="antol2015vqa" class="entry">
	<td>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C. and Parikh, D.</td>
	<td>Vqa: Visual question answering</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 2425-2433&nbsp;</td>
	<td>inproceedings</td>
	<td>313</td>
</tr>

<tr id="arel2010deep" class="entry">
	<td>Arel, I., Rose, D.C. and Karnowski, T.P.</td>
	<td>Deep machine learning-a new frontier in artificial intelligence research [research frontier]</td>
	
	<td>2010</td>
	<td>IEEE Computational Intelligence Magazine<br/>Vol. 5(4), pp. 13-18&nbsp;</td>
	<td>article</td>
	<td>405</td>
</tr>

<tr id="ba2016layer" class="entry">
	<td>Ba, J.L., Kiros, J.R. and Hinton, G.E.</td>
	<td>Layer normalization</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1607.06450&nbsp;</td>
	<td>article</td>
	<td>74</td>
</tr>

<tr id="bahdanau2014neural" class="entry">
	<td>Bahdanau, D., Cho, K. and Bengio, Y.</td>
	<td>Neural machine translation by jointly learning to align and translate</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1409.0473&nbsp;</td>
	<td>article</td>
	<td>1496</td>
</tr>

<tr id="bahdanau2016end" class="entry">
	<td>Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P. and Bengio, Y.</td>
	<td>End-to-end attention-based large vocabulary speech recognition</td>
	
	<td>2016</td>
	<td>Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 4945-4949&nbsp;</td>
	<td>inproceedings</td>
	<td>85</td>
</tr>

<tr id="Bengio-2009" class="entry">
	<td>Bengio, Y.</td>
	<td>Learning deep architectures for AI</td>
	
	<td>2009</td>
	<td>Foundations and Trends in Machine Learning<br/>Vol. 2(1), pp. 1-127&nbsp;</td>
	<td>article</td>
	<td>3836</td>
</tr>

<tr id="DBLP:journals/corr/abs-1206-5538" class="entry">
	<td>Bengio, Y., Courville, A.C. and Vincent, P.</td>
	<td>Unsupervised Feature Learning and Deep Learning: A Review and New<br> Perspectives</td>
	
	<td>2012</td>
	<td>CoRR<br/>Vol. abs/1206.5538&nbsp;</td>
	<td>article</td>
	<td>189</td>
</tr>

<tr id="bengio1994learning" class="entry">
	<td>Bengio, Y., Simard, P. and Frasconi, P.</td>
	<td>Learning long-term dependencies with gradient descent is difficult</td>
	
	<td>1994</td>
	<td>IEEE transactions on neural networks<br/>Vol. 5(2), pp. 157-166&nbsp;</td>
	<td>article</td>
	<td>1531</td>
</tr>

<tr id="bergstra2012random" class="entry">
	<td>Bergstra, J. and Bengio, Y.</td>
	<td>Random search for hyper-parameter optimization</td>
	
	<td>2012</td>
	<td>Journal of Machine Learning Research<br/>Vol. 13(Feb), pp. 281-305&nbsp;</td>
	<td>article</td>
	<td>892</td>
</tr>

<tr id="bordes2012joint" class="entry">
	<td>Bordes, A., Glorot, X., Weston, J. and Bengio, Y.</td>
	<td>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.</td>
	
	<td>2012</td>
	<td><br/>Vol. 22AISTATS, pp. 127-135&nbsp;</td>
	<td>inproceedings</td>
	<td>126</td>
</tr>

<tr id="chatfield2014return" class="entry">
	<td>Chatfield, K., Simonyan, K., Vedaldi, A. and Zisserman, A.</td>
	<td>Return of the devil in the details: Delving deep into convolutional nets</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1405.3531&nbsp;</td>
	<td>article</td>
	<td>962</td>
</tr>

<tr id="chen2014semantic" class="entry">
	<td>Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. and Yuille, A.L.</td>
	<td>Semantic image segmentation with deep convolutional nets and fully connected crfs</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.7062&nbsp;</td>
	<td>article</td>
	<td>619</td>
</tr>

<tr id="cho2014learning" class="entry">
	<td>Cho, K., Van Merri&euml;nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H. and Bengio, Y.</td>
	<td>Learning phrase representations using RNN encoder-decoder for statistical machine translation</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1406.1078&nbsp;</td>
	<td>article</td>
	<td>1060</td>
</tr>

<tr id="chung2016character" class="entry">
	<td>Chung, J., Cho, K. and Bengio, Y.</td>
	<td>A character-level decoder without explicit segmentation for neural machine translation</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1603.06147&nbsp;</td>
	<td>article</td>
	<td>63</td>
</tr>

<tr id="cirecsan2011committee" class="entry">
	<td>Cire&#351;an, D., Meier, U., Masci, J. and Schmidhuber, J&uuml;.</td>
	<td>A committee of neural networks for traffic sign classification</td>
	
	<td>2011</td>
	<td>Neural Networks (IJCNN), The 2011 International Joint Conference on, pp. 1918-1921&nbsp;</td>
	<td>inproceedings</td>
	<td>195</td>
</tr>

<tr id="ciregan2012multi" class="entry">
	<td>Ciregan, D., Meier, U. and Schmidhuber, J&uuml;.</td>
	<td>Multi-column deep neural networks for image classification</td>
	
	<td>2012</td>
	<td>Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642-3649&nbsp;</td>
	<td>inproceedings</td>
	<td>1117</td>
</tr>

<tr id="cirecsan2010deep" class="entry">
	<td>Ciresan, D.C., Meier, U., Gambardella, L.M. and Schmidhuber, J.</td>
	<td>Deep, big, simple neural nets for handwritten digit recognition</td>
	
	<td>2010</td>
	<td>Neural computation<br/>Vol. 22(12), pp. 3207-3220&nbsp;</td>
	<td>article</td>
	<td>432</td>
</tr>

<tr id="courbariaux2016binarized" class="entry">
	<td>Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R. and Bengio, Y.</td>
	<td>Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1602.02830&nbsp;</td>
	<td>article</td>
	<td>100</td>
</tr>

<tr id="dahl2012context" class="entry">
	<td>Dahl, G.E., Yu, D., Deng, L. and Acero, A.</td>
	<td>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</td>
	
	<td>2012</td>
	<td>IEEE Transactions on Audio, Speech, and Language Processing<br/>Vol. 20(1), pp. 30-42&nbsp;</td>
	<td>article</td>
	<td>1455</td>
</tr>

<tr id="donahue2015long" class="entry">
	<td>Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K. and Darrell, T.</td>
	<td>Long-term recurrent convolutional networks for visual recognition and description</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2625-2634&nbsp;</td>
	<td>inproceedings</td>
	<td>807</td>
</tr>

<tr id="donahue2014decaf" class="entry">
	<td>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E. and Darrell, T.</td>
	<td>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.</td>
	
	<td>2014</td>
	<td><br/>Vol. 32Icml, pp. 647-655&nbsp;</td>
	<td>inproceedings</td>
	<td>1389</td>
</tr>

<tr id="dong2016image" class="entry">
	<td>Dong, C., Loy, C.C., He, K. and Tang, X.</td>
	<td>Image super-resolution using deep convolutional networks</td>
	
	<td>2016</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 38(2), pp. 295-307&nbsp;</td>
	<td>article</td>
	<td>287</td>
</tr>

<tr id="farabet2013learning" class="entry">
	<td>Farabet, C., Couprie, C., Najman, L. and LeCun, Y.</td>
	<td>Learning hierarchical features for scene labeling</td>
	
	<td>2013</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 35(8), pp. 1915-1929&nbsp;</td>
	<td>article</td>
	<td>950</td>
</tr>

<tr id="ganin2016domain" class="entry">
	<td>Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M. and Lempitsky, V.</td>
	<td>Domain-adversarial training of neural networks</td>
	
	<td>2016</td>
	<td>Journal of Machine Learning Research<br/>Vol. 17(59), pp. 1-35&nbsp;</td>
	<td>article</td>
	<td>80</td>
</tr>

<tr id="gatys2015neural" class="entry">
	<td>Gatys, L.A., Ecker, A.S. and Bethge, M.</td>
	<td>A neural algorithm of artistic style</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1508.06576&nbsp;</td>
	<td>article</td>
	<td>229</td>
</tr>

<tr id="girshick2015fast" class="entry">
	<td>Girshick, R.</td>
	<td>Fast r-cnn</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448&nbsp;</td>
	<td>inproceedings</td>
	<td>1016</td>
</tr>

<tr id="girshick2016region" class="entry">
	<td>Girshick, R., Donahue, J., Darrell, T. and Malik, J.</td>
	<td>Region-based convolutional networks for accurate object detection and segmentation</td>
	
	<td>2016</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 38(1), pp. 142-158&nbsp;</td>
	<td>article</td>
	<td>173</td>
</tr>

<tr id="girshick2014rich" class="entry">
	<td>Girshick, R., Donahue, J., Darrell, T. and Malik, J.</td>
	<td>Rich feature hierarchies for accurate object detection and semantic segmentation</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587&nbsp;</td>
	<td>inproceedings</td>
	<td>3076</td>
</tr>

<tr id="goodfellow2014generative" class="entry">
	<td>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y.</td>
	<td>Generative adversarial nets</td>
	
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 2672-2680&nbsp;</td>
	<td>inproceedings</td>
	<td>830</td>
</tr>

<tr id="goodfellow2013maxout" class="entry">
	<td>Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A.C. and Bengio, Y.</td>
	<td>Maxout Networks</td>
	
	<td>2013</td>
	<td>ICML (3)<br/>Vol. 28, pp. 1319-1327&nbsp;</td>
	<td>article</td>
	<td>773</td>
</tr>

<tr id="graves2013generating" class="entry">
	<td>Graves, A.</td>
	<td>Generating sequences with recurrent neural networks</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1308.0850&nbsp;</td>
	<td>article</td>
	<td>594</td>
</tr>

<tr id="graves2012supervised" class="entry">
	<td>Graves, A.</td>
	<td>Supervised sequence labelling</td>
	
	<td>2012</td>
	<td>Supervised Sequence Labelling with Recurrent Neural Networks, pp. 5-13&nbsp;</td>
	<td>incollection</td>
	<td>532</td>
</tr>

<tr id="graves2006connectionist" class="entry">
	<td>Graves, A., Fern&aacute;ndez, S., Gomez, F. and Schmidhuber, J&uuml;.</td>
	<td>Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</td>
	
	<td>2006</td>
	<td>Proceedings of the 23rd international conference on Machine learning, pp. 369-376&nbsp;</td>
	<td>inproceedings</td>
	<td>452</td>
</tr>

<tr id="graves2009novel" class="entry">
	<td>Graves, A., Liwicki, M., Fern&aacute;ndez, S., Bertolami, R., Bunke, H. and Schmidhuber, J&uuml;.</td>
	<td>A novel connectionist system for unconstrained handwriting recognition</td>
	
	<td>2009</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 31(5), pp. 855-868&nbsp;</td>
	<td>article</td>
	<td>604</td>
</tr>

<tr id="graves2013speech" class="entry">
	<td>Graves, A., Mohamed, A.-r. and Hinton, G.</td>
	<td>Speech recognition with deep recurrent neural networks</td>
	
	<td>2013</td>
	<td>Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645-6649&nbsp;</td>
	<td>inproceedings</td>
	<td>1209</td>
</tr>

<tr id="graves2005framewise" class="entry">
	<td>Graves, A. and Schmidhuber, J&uuml;.</td>
	<td>Framewise phoneme classification with bidirectional LSTM and other neural network architectures</td>
	
	<td>2005</td>
	<td>Neural Networks<br/>Vol. 18(5), pp. 602-610&nbsp;</td>
	<td>article</td>
	<td>502</td>
</tr>

<tr id="graves2014neural" class="entry">
	<td>Graves, A., Wayne, G. and Danihelka, I.</td>
	<td>Neural turing machines</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1410.5401&nbsp;</td>
	<td>article</td>
	<td>353</td>
</tr>

<tr id="graves2016hybrid" class="entry">
	<td>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi&#324;ska, A., Colmenarejo, S.G&oacute;., Grefenstette, E., Ramalho, T., Agapiou, J. and others</td>
	<td>Hybrid computing using a neural network with dynamic external memory</td>
	
	<td>2016</td>
	<td>Nature<br/>Vol. 538(7626), pp. 471-476&nbsp;</td>
	<td>article</td>
	<td>87</td>
</tr>

<tr id="gregor2015draw" class="entry">
	<td>Gregor, K., Danihelka, I., Graves, A., Rezende, D.J. and Wierstra, D.</td>
	<td>DRAW: A recurrent neural network for image generation</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1502.04623&nbsp;</td>
	<td>article</td>
	<td>385</td>
</tr>

<tr id="han2016eie" class="entry">
	<td>Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A. and Dally, W.J.</td>
	<td>EIE: efficient inference engine on compressed deep neural network</td>
	
	<td>2016</td>
	<td>Proceedings of the 43rd International Symposium on Computer Architecture, pp. 243-254&nbsp;</td>
	<td>inproceedings</td>
	<td>96</td>
</tr>

<tr id="he2016deep" class="entry">
	<td>He, K., Zhang, X., Ren, S. and Sun, J.</td>
	<td>Deep residual learning for image recognition</td>
	
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778&nbsp;</td>
	<td>inproceedings</td>
	<td>2345</td>
</tr>

<tr id="he2016identity" class="entry">
	<td>He, K., Zhang, X., Ren, S. and Sun, J.</td>
	<td>Identity mappings in deep residual networks</td>
	
	<td>2016</td>
	<td>European Conference on Computer Vision, pp. 630-645&nbsp;</td>
	<td>inproceedings</td>
	<td>277</td>
</tr>

<tr id="he2015delving" class="entry">
	<td>He, K., Zhang, X., Ren, S. and Sun, J.</td>
	<td>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 1026-1034&nbsp;</td>
	<td>inproceedings</td>
	<td>1106</td>
</tr>

<tr id="he2014spatial" class="entry">
	<td>He, K., Zhang, X., Ren, S. and Sun, J.</td>
	<td>Spatial pyramid pooling in deep convolutional networks for visual recognition</td>
	
	<td>2014</td>
	<td>European Conference on Computer Vision, pp. 346-361&nbsp;</td>
	<td>inproceedings</td>
	<td>606</td>
</tr>

<tr id="hermann2015teaching" class="entry">
	<td>Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M. and Blunsom, P.</td>
	<td>Teaching machines to read and comprehend</td>
	
	<td>2015</td>
	<td>Advances in Neural Information Processing Systems, pp. 1693-1701&nbsp;</td>
	<td>inproceedings</td>
	<td>261</td>
</tr>

<tr id="hinton2010practical" class="entry">
	<td>Hinton, G.</td>
	<td>A practical guide to training restricted Boltzmann machines</td>
	
	<td>2010</td>
	<td>Momentum<br/>Vol. 9(1), pp. 926&nbsp;</td>
	<td>article</td>
	<td>984</td>
</tr>

<tr id="hinton2012deep" class="entry">
	<td>Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others</td>
	<td>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</td>
	
	<td>2012</td>
	<td>IEEE Signal Processing Magazine<br/>Vol. 29(6), pp. 82-97&nbsp;</td>
	<td>article</td>
	<td>2746</td>
</tr>

<tr id="hinton2015distilling" class="entry">
	<td>Hinton, G., Vinyals, O. and Dean, J.</td>
	<td>Distilling the knowledge in a neural network</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1503.02531&nbsp;</td>
	<td>article</td>
	<td>340</td>
</tr>

<tr id="hinton2012improving" class="entry">
	<td>Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R.R.</td>
	<td>Improving neural networks by preventing co-adaptation of feature detectors</td>
	
	<td>2012</td>
	<td>arXiv preprint arXiv:1207.0580&nbsp;</td>
	<td>article</td>
	<td>1906</td>
</tr>

<tr id="hochreiter2001gradient" class="entry">
	<td>Hochreiter, S., Bengio, Y., Frasconi, P. and Schmidhuber, J&uuml;.</td>
	<td>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</td>
	
	<td>2001</td>
	<td>&nbsp;</td>
	<td>misc</td>
	<td>402</td>
</tr>

<tr id="hochreiter1997long" class="entry">
	<td>Hochreiter, S. and Schmidhuber, J&uuml;.</td>
	<td>Long short-term memory</td>
	
	<td>1997</td>
	<td>Neural computation<br/>Vol. 9(8), pp. 1735-1780&nbsp;</td>
	<td>article</td>
	<td>4453</td>
</tr>

<tr id="iandola2016squeezenet" class="entry">
	<td>Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J. and Keutzer, K.</td>
	<td>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1602.07360&nbsp;</td>
	<td>article</td>
	<td>100</td>
</tr>

<tr id="ioffe2015batch" class="entry">
	<td>Ioffe, S. and Szegedy, C.</td>
	<td>Batch normalization: Accelerating deep network training by reducing internal covariate shift</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1502.03167&nbsp;</td>
	<td>article</td>
	<td>1719</td>
</tr>

<tr id="ji20133d" class="entry">
	<td>Ji, S., Xu, W., Yang, M. and Yu, K.</td>
	<td>3D convolutional neural networks for human action recognition</td>
	
	<td>2013</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 35(1), pp. 221-231&nbsp;</td>
	<td>article</td>
	<td>851</td>
</tr>

<tr id="jozefowicz2016exploring" class="entry">
	<td>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N. and Wu, Y.</td>
	<td>Exploring the limits of language modeling</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1602.02410&nbsp;</td>
	<td>article</td>
	<td>102</td>
</tr>

<tr id="kalchbrenner2014convolutional" class="entry">
	<td>Kalchbrenner, N., Grefenstette, E. and Blunsom, P.</td>
	<td>A convolutional neural network for modelling sentences</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1404.2188&nbsp;</td>
	<td>article</td>
	<td>615</td>
</tr>

<tr id="karpathy2015deep" class="entry">
	<td>Karpathy, A. and Fei-Fei, L.</td>
	<td>Deep visual-semantic alignments for generating image descriptions</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128-3137&nbsp;</td>
	<td>inproceedings</td>
	<td>862</td>
</tr>

<tr id="karpathy2014large" class="entry">
	<td>Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R. and Fei-Fei, L.</td>
	<td>Large-scale video classification with convolutional neural networks</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1725-1732&nbsp;</td>
	<td>inproceedings</td>
	<td>1123</td>
</tr>

<tr id="kavukcuoglu2010learning" class="entry">
	<td>Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M. and Cun, Y.L.</td>
	<td>Learning convolutional feature hierarchies for visual recognition</td>
	
	<td>2010</td>
	<td>Advances in neural information processing systems, pp. 1090-1098&nbsp;</td>
	<td>inproceedings</td>
	<td>350</td>
</tr>

<tr id="kim2014convolutional" class="entry">
	<td>Kim, Y.</td>
	<td>Convolutional neural networks for sentence classification</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1408.5882&nbsp;</td>
	<td>article</td>
	<td>875</td>
</tr>

<tr id="kingma2014adam" class="entry">
	<td>Kingma, D. and Ba, J.</td>
	<td>Adam: A method for stochastic optimization</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.6980&nbsp;</td>
	<td>article</td>
	<td>2660</td>
</tr>

<tr id="kingma2013auto" class="entry">
	<td>Kingma, D.P. and Welling, M.</td>
	<td>Auto-encoding variational bayes</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1312.6114&nbsp;</td>
	<td>article</td>
	<td>860</td>
</tr>

<tr id="krizhevsky2012imagenet" class="entry">
	<td>Krizhevsky, A., Sutskever, I. and Hinton, G.E.</td>
	<td>Imagenet classification with deep convolutional neural networks</td>
	
	<td>2012</td>
	<td>Advances in neural information processing systems, pp. 1097-1105&nbsp;</td>
	<td>inproceedings</td>
	<td>11904</td>
</tr>

<tr id="le2013building" class="entry">
	<td>Le, Q.V.</td>
	<td>Building high-level features using large scale unsupervised learning</td>
	
	<td>2013</td>
	<td>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595-8598&nbsp;</td>
	<td>inproceedings</td>
	<td>1252</td>
</tr>

<tr id="le2014distributed" class="entry">
	<td>Le, Q.V. and Mikolov, T.</td>
	<td>Distributed Representations of Sentences and Documents</td>
	
	<td>2014</td>
	<td><br/>Vol. 14ICML, pp. 1188-1196&nbsp;</td>
	<td>inproceedings</td>
	<td>1160</td>
</tr>

<tr id="lecun2015deep" class="entry">
	<td>LeCun, Y., Bengio, Y. and Hinton, G.</td>
	<td>Deep learning</td>
	
	<td>2015</td>
	<td>Nature<br/>Vol. 521(7553), pp. 436-444&nbsp;</td>
	<td>article</td>
	<td>2583</td>
</tr>

<tr id="lenz2015deep" class="entry">
	<td>Lenz, I., Lee, H. and Saxena, A.</td>
	<td>Deep learning for detecting robotic grasps</td>
	
	<td>2015</td>
	<td>The International Journal of Robotics Research<br/>Vol. 34(4-5), pp. 705-724&nbsp;</td>
	<td>article</td>
	<td>219</td>
</tr>

<tr id="levine2016end" class="entry">
	<td>Levine, S., Finn, C., Darrell, T. and Abbeel, P.</td>
	<td>End-to-end training of deep visuomotor policies</td>
	
	<td>2016</td>
	<td>Journal of Machine Learning Research<br/>Vol. 17(39), pp. 1-40&nbsp;</td>
	<td>article</td>
	<td>262</td>
</tr>

<tr id="levine2016learning" class="entry">
	<td>Levine, S., Pastor, P., Krizhevsky, A. and Quillen, D.</td>
	<td>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1603.02199&nbsp;</td>
	<td>article</td>
	<td>88</td>
</tr>

<tr id="lillicrap2015continuous" class="entry">
	<td>Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D.</td>
	<td>Continuous control with deep reinforcement learning</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1509.02971&nbsp;</td>
	<td>article</td>
	<td>209</td>
</tr>

<tr id="lin2013network" class="entry">
	<td>Lin, M., Chen, Q. and Yan, S.</td>
	<td>Network in network</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1312.4400&nbsp;</td>
	<td>article</td>
	<td>727</td>
</tr>

<tr id="liu2016ssd" class="entry">
	<td>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y. and Berg, A.C.</td>
	<td>SSD: Single shot multibox detector</td>
	
	<td>2016</td>
	<td>European Conference on Computer Vision, pp. 21-37&nbsp;</td>
	<td>inproceedings</td>
	<td>189</td>
</tr>

<tr id="long2015fully" class="entry">
	<td>Long, J., Shelhamer, E. and Darrell, T.</td>
	<td>Fully convolutional networks for semantic segmentation</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440&nbsp;</td>
	<td>inproceedings</td>
	<td>1801</td>
</tr>

<tr id="luong2015effective" class="entry">
	<td>Luong, M.-T., Pham, H. and Manning, C.D.</td>
	<td>Effective approaches to attention-based neural machine translation</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1508.04025&nbsp;</td>
	<td>article</td>
	<td>296</td>
</tr>

<tr id="mikolov2012statistical" class="entry">
	<td>Mikolov, T.</td>
	<td>Statistical language models based on neural networks</td>
	
	<td>2012</td>
	<td>Presentation at Google, Mountain View, 2nd April&nbsp;</td>
	<td>article</td>
	<td>311</td>
</tr>

<tr id="mikolov2013efficient" class="entry">
	<td>Mikolov, T., Chen, K., Corrado, G. and Dean, J.</td>
	<td>Efficient estimation of word representations in vector space</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1301.3781&nbsp;</td>
	<td>article</td>
	<td>3548</td>
</tr>

<tr id="mikolov2013distributed" class="entry">
	<td>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J.</td>
	<td>Distributed representations of words and phrases and their compositionality</td>
	
	<td>2013</td>
	<td>Advances in neural information processing systems, pp. 3111-3119&nbsp;</td>
	<td>inproceedings</td>
	<td>3883</td>
</tr>

<tr id="mnih2016asynchronous" class="entry">
	<td>Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D. and Kavukcuoglu, K.</td>
	<td>Asynchronous methods for deep reinforcement learning</td>
	
	<td>2016</td>
	<td>International Conference on Machine Learning&nbsp;</td>
	<td>inproceedings</td>
	<td>192</td>
</tr>

<tr id="mnih2014recurrent" class="entry">
	<td>Mnih, V., Heess, N., Graves, A. and others</td>
	<td>Recurrent models of visual attention</td>
	
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 2204-2212&nbsp;</td>
	<td>inproceedings</td>
	<td>317</td>
</tr>

<tr id="mnih2013playing" class="entry">
	<td>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M.</td>
	<td>Playing atari with deep reinforcement learning</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1312.5602&nbsp;</td>
	<td>article</td>
	<td>521</td>
</tr>

<tr id="mnih2015human" class="entry">
	<td>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and others</td>
	<td>Human-level control through deep reinforcement learning</td>
	
	<td>2015</td>
	<td>Nature<br/>Vol. 518(7540), pp. 529-533&nbsp;</td>
	<td>article</td>
	<td>1198</td>
</tr>

<tr id="mohamed2012acoustic" class="entry">
	<td>Mohamed, A.-r., Dahl, G.E. and Hinton, G.</td>
	<td>Acoustic modeling using deep belief networks</td>
	
	<td>2012</td>
	<td>IEEE Transactions on Audio, Speech, and Language Processing<br/>Vol. 20(1), pp. 14-22&nbsp;</td>
	<td>article</td>
	<td>950</td>
</tr>

<tr id="nguyen2015deep" class="entry">
	<td>Nguyen, A., Yosinski, J. and Clune, J.</td>
	<td>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427-436&nbsp;</td>
	<td>inproceedings</td>
	<td>339</td>
</tr>

<tr id="oord2016pixel" class="entry">
	<td>Oord, A.v.d., Kalchbrenner, N. and Kavukcuoglu, K.</td>
	<td>Pixel recurrent neural networks</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1601.06759&nbsp;</td>
	<td>article</td>
	<td>140</td>
</tr>

<tr id="van2016wavenet" class="entry">
	<td>van den Oord, A&auml;., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. and Kavukcuoglu, K.</td>
	<td>Wavenet: A generative model for raw audio</td>
	
	<td>2016</td>
	<td>CoRR abs/1609.03499&nbsp;</td>
	<td>article</td>
	<td>46</td>
</tr>

<tr id="oquab2014learning" class="entry">
	<td>Oquab, M., Bottou, L., Laptev, I. and Sivic, J.</td>
	<td>Learning and transferring mid-level image representations using convolutional neural networks</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1717-1724&nbsp;</td>
	<td>inproceedings</td>
	<td>656</td>
</tr>

<tr id="pennington2014glove" class="entry">
	<td>Pennington, J., Socher, R. and Manning, C.D.</td>
	<td>Glove: Global Vectors for Word Representation</td>
	
	<td>2014</td>
	<td><br/>Vol. 14EMNLP, pp. 1532-1543&nbsp;</td>
	<td>inproceedings</td>
	<td>1849</td>
</tr>

<tr id="radford2015unsupervised" class="entry">
	<td>Radford, A., Metz, L. and Chintala, S.</td>
	<td>Unsupervised representation learning with deep convolutional generative adversarial networks</td>
	
	<td>2015</td>
	<td>arXiv preprint arXiv:1511.06434&nbsp;</td>
	<td>article</td>
	<td>423</td>
</tr>

<tr id="redmon2016you" class="entry">
	<td>Redmon, J., Divvala, S., Girshick, R. and Farhadi, A.</td>
	<td>You only look once: Unified, real-time object detection</td>
	
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788&nbsp;</td>
	<td>inproceedings</td>
	<td>302</td>
</tr>

<tr id="ren2015faster" class="entry">
	<td>Ren, S., He, K., Girshick, R. and Sun, J.</td>
	<td>Faster r-cnn: Towards real-time object detection with region proposal networks</td>
	
	<td>2015</td>
	<td>Advances in neural information processing systems, pp. 91-99&nbsp;</td>
	<td>inproceedings</td>
	<td>1223</td>
</tr>

<tr id="salakhutdinov2010efficient" class="entry">
	<td>Salakhutdinov, R. and Larochelle, H.</td>
	<td>Efficient Learning of Deep Boltzmann Machines.</td>
	
	<td>2010</td>
	<td><br/>Vol. 9AISTATs, pp. 693-700&nbsp;</td>
	<td>inproceedings</td>
	<td>299</td>
</tr>

<tr id="salimans2016improved" class="entry">
	<td>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X.</td>
	<td>Improved techniques for training gans</td>
	
	<td>2016</td>
	<td>Advances in Neural Information Processing Systems, pp. 2226-2234&nbsp;</td>
	<td>inproceedings</td>
	<td>187</td>
</tr>

<tr id="888" class="entry">
	<td>Schmidhuber, J.</td>
	<td>Deep Learning in Neural Networks: An Overview</td>
	
	<td>2015</td>
	<td>Neural Networks<br/>Vol. 61, pp. 85-117&nbsp;</td>
	<td>article</td>
	<td>1337</td>
</tr>

<tr id="schmidhuber1992learning" class="entry">
	<td>Schmidhuber, J&uuml;.</td>
	<td>Learning complex, extended sequences using the principle of history compression</td>
	
	<td>1992</td>
	<td>Neural Computation<br/>Vol. 4(2), pp. 234-242&nbsp;</td>
	<td>article</td>
	<td>211</td>
</tr>

<tr id="sermanet2013overfeat" class="entry">
	<td>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R. and LeCun, Y.</td>
	<td>Overfeat: Integrated recognition, localization and detection using convolutional networks</td>
	
	<td>2013</td>
	<td>arXiv preprint arXiv:1312.6229&nbsp;</td>
	<td>article</td>
	<td>1424</td>
</tr>

<tr id="sharif2014cnn" class="entry">
	<td>Sharif Razavian, A., Azizpour, H., Sullivan, J. and Carlsson, S.</td>
	<td>CNN features off-the-shelf: an astounding baseline for recognition</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 806-813&nbsp;</td>
	<td>inproceedings</td>
	<td>1209</td>
</tr>

<tr id="silver2016mastering" class="entry">
	<td>Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M. and others</td>
	<td>Mastering the game of Go with deep neural networks and tree search</td>
	
	<td>2016</td>
	<td>Nature<br/>Vol. 529(7587), pp. 484-489&nbsp;</td>
	<td>article</td>
	<td>1709</td>
</tr>

<tr id="simonyan2014two" class="entry">
	<td>Simonyan, K. and Zisserman, A.</td>
	<td>Two-stream convolutional networks for action recognition in videos</td>
	
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 568-576&nbsp;</td>
	<td>inproceedings</td>
	<td>754</td>
</tr>

<tr id="simonyan2014very" class="entry">
	<td>Simonyan, K. and Zisserman, A.</td>
	<td>Very deep convolutional networks for large-scale image recognition</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1409.1556&nbsp;</td>
	<td>article</td>
	<td>4799</td>
</tr>

<tr id="socher2011dynamic" class="entry">
	<td>Socher, R., Huang, E.H., Pennington, J., Ng, A.Y. and Manning, C.D.</td>
	<td>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</td>
	
	<td>2011</td>
	<td><br/>Vol. 24NIPS, pp. 801-809&nbsp;</td>
	<td>inproceedings</td>
	<td>415</td>
</tr>

<tr id="socher2011semi" class="entry">
	<td>Socher, R., Pennington, J., Huang, E.H., Ng, A.Y. and Manning, C.D.</td>
	<td>Semi-supervised recursive autoencoders for predicting sentiment distributions</td>
	
	<td>2011</td>
	<td>Proceedings of the conference on empirical methods in natural language processing, pp. 151-161&nbsp;</td>
	<td>inproceedings</td>
	<td>581</td>
</tr>

<tr id="socher2013recursive" class="entry">
	<td>Socher, R., Perelygin, A., Wu, J.Y., Chuang, J., Manning, C.D., Ng, A.Y., Potts, C. and others</td>
	<td>Recursive deep models for semantic compositionality over a sentiment treebank</td>
	
	<td>2013</td>
	<td><br/>Vol. 1631Proceedings of the conference on empirical methods in natural language processing (EMNLP), pp. 1642&nbsp;</td>
	<td>inproceedings</td>
	<td>1284</td>
</tr>

<tr id="srivastava2014dropout" class="entry">
	<td>Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R.</td>
	<td>Dropout: a simple way to prevent neural networks from overfitting</td>
	
	<td>2014</td>
	<td>Journal of Machine Learning Research<br/>Vol. 15(1), pp. 1929-1958&nbsp;</td>
	<td>article</td>
	<td>2731</td>
</tr>

<tr id="sutskever2013training" class="entry">
	<td>Sutskever, I.</td>
	<td>Training recurrent neural networks</td>
	
	<td>2013</td>
	<td><i>School</i>: University of Toronto&nbsp;</td>
	<td>phdthesis</td>
	<td>127</td>
</tr>

<tr id="sutskever2014sequence" class="entry">
	<td>Sutskever, I., Vinyals, O. and Le, Q.V.</td>
	<td>Sequence to sequence learning with neural networks</td>
	
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 3104-3112&nbsp;</td>
	<td>inproceedings</td>
	<td>1621</td>
</tr>

<tr id="szegedy2016inception" class="entry">
	<td>Szegedy, C., Ioffe, S., Vanhoucke, V. and Alemi, A.</td>
	<td>Inception-v4, inception-resnet and the impact of residual connections on learning</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1602.07261&nbsp;</td>
	<td>article</td>
	<td>163</td>
</tr>

<tr id="szegedy2015going" class="entry">
	<td>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A.</td>
	<td>Going deeper with convolutions</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9&nbsp;</td>
	<td>inproceedings</td>
	<td>3223</td>
</tr>

<tr id="szegedy2016rethinking" class="entry">
	<td>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. and Wojna, Z.</td>
	<td>Rethinking the inception architecture for computer vision</td>
	
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826&nbsp;</td>
	<td>inproceedings</td>
	<td>312</td>
</tr>

<tr id="taigman2014deepface" class="entry">
	<td>Taigman, Y., Yang, M., Ranzato, M. and Wolf, L.</td>
	<td>Deepface: Closing the gap to human-level performance in face verification</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1701-1708&nbsp;</td>
	<td>inproceedings</td>
	<td>1365</td>
</tr>

<tr id="toshev2014deeppose" class="entry">
	<td>Toshev, A. and Szegedy, C.</td>
	<td>Deeppose: Human pose estimation via deep neural networks</td>
	
	<td>2014</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1653-1660&nbsp;</td>
	<td>inproceedings</td>
	<td>436</td>
</tr>

<tr id="ulyanov2016texture" class="entry">
	<td>Ulyanov, D., Lebedev, V., Vedaldi, A. and Lempitsky, V.</td>
	<td>Texture networks: Feed-forward synthesis of textures and stylized images</td>
	
	<td>2016</td>
	<td>Int. Conf. on Machine Learning (ICML)&nbsp;</td>
	<td>inproceedings</td>
	<td>64</td>
</tr>

<tr id="van2016deep" class="entry">
	<td>Van Hasselt, H., Guez, A. and Silver, D.</td>
	<td>Deep Reinforcement Learning with Double Q-Learning</td>
	
	<td>2016</td>
	<td>AAAI, pp. 2094-2100&nbsp;</td>
	<td>inproceedings</td>
	<td>130</td>
</tr>

<tr id="vincent2010stacked" class="entry">
	<td>Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y. and Manzagol, P.-A.</td>
	<td>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</td>
	
	<td>2010</td>
	<td>Journal of Machine Learning Research<br/>Vol. 11(Dec), pp. 3371-3408&nbsp;</td>
	<td>article</td>
	<td>1507</td>
</tr>

<tr id="vinyals2015show" class="entry">
	<td>Vinyals, O., Toshev, A., Bengio, S. and Erhan, D.</td>
	<td>Show and tell: A neural image caption generator</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156-3164&nbsp;</td>
	<td>inproceedings</td>
	<td>892</td>
</tr>

<tr id="von1994correlation" class="entry">
	<td>Von Der Malsburg, C.</td>
	<td>The correlation theory of brain function</td>
	
	<td>1994</td>
	<td>Models of neural networks, pp. 95-119&nbsp;</td>
	<td>incollection</td>
	<td>1491</td>
</tr>

<tr id="weston2014memory" class="entry">
	<td>Weston, J., Chopra, S. and Bordes, A.</td>
	<td>Memory networks</td>
	
	<td>2014</td>
	<td>arXiv preprint arXiv:1410.3916&nbsp;</td>
	<td>article</td>
	<td>307</td>
</tr>

<tr id="wu2016google" class="entry">
	<td>Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K. and others</td>
	<td>Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</td>
	
	<td>2016</td>
	<td>arXiv preprint arXiv:1609.08144&nbsp;</td>
	<td>article</td>
	<td>116</td>
</tr>

<tr id="xiong2016dynamic" class="entry">
	<td>Xiong, C., Merity, S. and Socher, R.</td>
	<td>Dynamic memory networks for visual and textual question answering</td>
	
	<td>2016</td>
	<td>arXiv<br/>Vol. 1603&nbsp;</td>
	<td>article</td>
	<td>105</td>
</tr>

<tr id="xu2015show" class="entry">
	<td>Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S. and Bengio, Y.</td>
	<td>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</td>
	
	<td>2015</td>
	<td><br/>Vol. 14ICML, pp. 77-81&nbsp;</td>
	<td>inproceedings</td>
	<td>789</td>
</tr>

<tr id="yang2016stacked" class="entry">
	<td>Yang, Z., He, X., Gao, J., Deng, L. and Smola, A.</td>
	<td>Stacked attention networks for image question answering</td>
	
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 21-29&nbsp;</td>
	<td>inproceedings</td>
	<td>122</td>
</tr>

<tr id="yosinski2014transferable" class="entry">
	<td>Yosinski, J., Clune, J., Bengio, Y. and Lipson, H.</td>
	<td>How transferable are features in deep neural networks?</td>
	
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 3320-3328&nbsp;</td>
	<td>inproceedings</td>
	<td>611</td>
</tr>

<tr id="zeiler2014visualizing" class="entry">
	<td>Zeiler, M.D. and Fergus, R.</td>
	<td>Visualizing and understanding convolutional networks</td>
	
	<td>2014</td>
	<td>European conference on computer vision, pp. 818-833&nbsp;</td>
	<td>inproceedings</td>
	<td>1996</td>
</tr>

<tr id="zhang2016colorful" class="entry">
	<td>Zhang, R., Isola, P. and Efros, A.A.</td>
	<td>Colorful image colorization</td>
	
	<td>2016</td>
	<td>European Conference on Computer Vision, pp. 649-666&nbsp;</td>
	<td>inproceedings</td>
	<td>73</td>
</tr>

<tr id="zheng2015conditional" class="entry">
	<td>Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C. and Torr, P.H.</td>
	<td>Conditional random fields as recurrent neural networks</td>
	
	<td>2015</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 1529-1537&nbsp;</td>
	<td>inproceedings</td>
	<td>514</td>
</tr>

<tr id="zhu2016generative" class="entry">
	<td>Zhu, J.-Y., Kr&auml;henb&uuml;hl, P., Shechtman, E. and Efros, A.A.</td>
	<td>Generative visual manipulation on the natural image manifold</td>
	
	<td>2016</td>
	<td>European Conference on Computer Vision, pp. 597-613&nbsp;</td>
	<td>inproceedings</td>
	<td>47</td>
</tr>

